{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:powderblue;\"><h1 align=\"center\">Projet Synthese</h1>\n",
    "    <p><b>Groupe 3: Amardeepkumar Haulkhory, Hilaire Yuma, Jian-Guo Zhao, Mamadou Sy</b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Partie II Apprentissage par <abbr title=\"Convolutional Neural Networks\"><b>CNN </b></abbr></h2>\n",
    "<p>Definir le <abbr title=\"Convolutional Neural Networks\"><b>CNN </b></abbr>architecture:</p>\n",
    "<ol>\n",
    "    <li>La premiere couche, Conv2D, prend un batch de données avec input_shape=(48, 48, 1)</li>\n",
    "    <li>La deuxieme couche, Conv2D, prend la sortie de couche 1, keras determine le shape automatiquement</li>\n",
    "    <li>Il n'est pas necessaire d'indiquer input_shape pour les couches plus profondes</li>\n",
    "    <li>La troisième couche, Conv2D, prend la sortie de la couche 2, keras determine le shape automatiquement</li>\n",
    "    <li>Ainsi de suite, on peut ajouter autres couches Conv2D s'il est nécessaire</li>\n",
    "    <li>Entre chaque deux couches Conv2D, une couche Max pooling est ajoutée</li>\n",
    "    <li>Pour reduire le sur-apprentissage, une couche de regularisation Dropout pourrait etre ajoutée</li>\n",
    "    <li>Pour obtension d'une meilleure performance, une couche Normalisation pourrait etre aussi appliquée</li>\n",
    "    <li>Avant la couche dense, nous ajoutons une couche Flatten qui convert le matrix 2D en vector 1D</li>\n",
    "    <li>Puis, une couche Dense avec une fonction activation ReLu</li>\n",
    "    <li>Encore une couche de regularization Dropout est appliquée avec une valeur plus grande normallement</li>\n",
    "    <li>Enfin, la couche de sortie donne 7 neurons pour les 7 classes avec une fonction activation softmax qui donne la possibilité de prediction pour chaque class.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Dépendences - packages nécessaires</u></h3>\n",
    "<p>En utilisant <b>tensorflow.compat.v1</b>, vous devez désactiver <b>eager_execution</b>.<br>Ce n'est pas le cas pour <b>tensorflow.v2</b>. De plus, le package <b>tensorflow</b> n'a pas la fonction <b><i>disable_eager_execution</i></b>.</p><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_eager_execution()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.utils.data_utils import Sequence\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#from imblearn.keras import balanced_batch_generator\n",
    "\n",
    "import numpy as np      # linear algebra\n",
    "import pandas as pd     # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chargement des données</h3>\n",
    "<p>Nous recommendons de télécharger les données <a href=\"https://www.kaggle.com/deadskull7/fer2013\">fer2013</a> dans un répertoire local sur votre machine.<br>Ensuite inscrire le répertoire commme valeur de la variable <b>data_dir</b> ci-dessous.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\GitHUB\\\\prosyn3_files\\\\fer2013.csv'\n",
    "#\"C:\\\\Users\\\\jeang\\\\Documents\\\\BDEB\\\\A62\\\\ProSyn3\\\\data\\\\fer2013.csv\"\n",
    "df = pd.read_csv(data_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Répartition des données</h3>\n",
    "<p>Les données originales sont réparties en trois parties. Selons Partie I, la cible est bien balancée dans ces trois parties</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df[df['Usage']=='Training']\n",
    "df_validation = df[df['Usage']=='PublicTest']\n",
    "df_test = df[df['Usage']=='PrivateTest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Affichage des données</h3>\n",
    "<p> Ces fonctions affichent un image (une ligne)<br>Notons que 48 x 48 = 2304</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "FER2013_WIDTH = 48\n",
    "FER2013_HEIGHT = 48\n",
    "\n",
    "# indices 0 - 6 correspondent emotions suivantes\n",
    "Emotions = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]  \n",
    "\n",
    "def string_to_image(str_in):\n",
    "    return np.reshape(str_in.split(\" \"), (FER2013_WIDTH, FER2013_HEIGHT)).astype(\"float\")\n",
    "\n",
    "def fer2013_show_instance(index, d=df):\n",
    "    \"\"\"Shows the image and the emotion label of the index's instance.\"\"\"\n",
    "    category, str_image = d.at[index,'emotion'], d.at[index,'pixels']\n",
    "    image = string_to_image(str_image)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(f\"Exemple de categorie {category}: {Emotions[category]}\")\n",
    "    \n",
    "fer2013_show_instance(np.random.randint(0,len(df_training)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformation des données</h3>\n",
    "<p>La fonction <b><i>image_to_4d_array</i></b> prend comme entrée une dataframe contenant les données d'images en format des chaines de caractères.<br>Elle retoune comme sortie un array du genre <b>np.array</b> de <mark>4 dimensions</mark>, normalisée, x, et categoriel y, utilisables comme entrée pour <b>tensorflow CNN</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_4d_array(d=df, nclass=7):\n",
    "    \"\"\"Transforms the (blank separated) pixel strings in the DataFrame to an 4-dimensional array \n",
    "    (1st dim: instances, 2nd and 3rd dims represent 2D image, 4th dim represent the color).\"\"\"\n",
    "    \n",
    "    CLASS_NUMBER = min(nclass, 7)\n",
    "    \n",
    "    print(\"Converting pixels to 2D image ...\")\n",
    "    pixels_list = d[\"pixels\"].values\n",
    "    list_image_2d = [string_to_image(pixels) for pixels in pixels_list]\n",
    "        \n",
    "    # Convert list to 4D array:\n",
    "    X = np.expand_dims(np.array(list_image_2d), -1)\n",
    "    X = X / 255  \n",
    "    \n",
    "    print(\"Converting emotion to categorical n-array ...\")\n",
    "    Y = to_categorical(d.emotion, CLASS_NUMBER)\n",
    "    \n",
    "    print(f\"The given dataset has been converted to {X.shape} array\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>On applique la fonction <b><i>image_to_4d_array</i></b> sur les trois partitions</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting pixels to 2D image ...\n",
      "Converting emotion to categorical n-array ...\n",
      "The given dataset has been converted to (28709, 48, 48, 1) array\n",
      "Converting pixels to 2D image ...\n",
      "Converting emotion to categorical n-array ...\n",
      "The given dataset has been converted to (3589, 48, 48, 1) array\n",
      "Converting pixels to 2D image ...\n",
      "Converting emotion to categorical n-array ...\n",
      "The given dataset has been converted to (3589, 48, 48, 1) array\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = image_to_4d_array(df_training)\n",
    "X_test, Y_test = image_to_4d_array(df_test)\n",
    "X_valid, Y_valid = image_to_4d_array(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fonction <i>get_model_v23</i></h3>\n",
    "<ul>\n",
    "<li>Cette fonction <b style=\"backgroud-color:powerblue\"><i>get_model_v23</i></b> peut prendre seulement quelques hyperparamètres</li>\n",
    "<li>Elle appelle <i style=\"color:blue\">keras.models.Sequential</i> qui nous permet de construire un modèle cnn</li>\n",
    "<li>Les valeurs par défaut sont notre choix entre les meilleures, mais elle laisse des chances pour utilisateur de les choisir</li>\n",
    "<li>Elle retourne un architecture d'un modèle sequential qu'on peut entrainer et tester</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "<h4>Points essentiels de la fonction <i>get_model_v23</i></h4>\n",
    "<ol>\n",
    "<li>Nombre de couches est 5 par défaut, mais elle peut être un entier de votre choix</li>\n",
    "<li>Les nombres de filtre doivent être en rapport avec le nombre de couche, de préference plus elevé pour les couches profondes.</li>\n",
    "<li>Les kernel sizes doivent être en rapport avec le nombre de couches, de préference impaire et le plus petit que possible</li>\n",
    "<li>Les pooling size doivent être en rapport avec le nombre de couches, de préference petit ou zéro </li>\n",
    "<li>Les dropout values doivent être en rapport avec le nombre de couches.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v23(num_layers=5, lf=256, ld=0.5,\n",
    "                  num_filtre=(16, 32, 64, 128, 256), \n",
    "                  k_size=(3,3,3,3,3), \n",
    "                  p_size=(2,2,2,2,0), \n",
    "                  dropout=(0.10, 0.10, 0.15, 0.10, 0.10)):\n",
    "    \n",
    "    input_shape, num_classes, last_features, last_dropout = (48, 48, 1), 7, lf, ld\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for layer in range(num_layers): \n",
    "        if layer == 0:\n",
    "            model.add(Conv2D(num_filtre[layer], kernel_size=k_size[layer], activation=tf.nn.relu, padding=\"same\", input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(Conv2D(num_filtre[layer], kernel_size=k_size[layer], activation=tf.nn.relu, padding=\"same\"))\n",
    "            # normalization for each layer but not the first\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # max pooling with the given choose\n",
    "        if p_size[layer] > 0:\n",
    "            model.add(MaxPooling2D(pool_size=(p_size[layer], p_size[layer])))\n",
    "        \n",
    "        # Dropout with the given choose\n",
    "        if dropout[layer] > 0:\n",
    "            model.add(Dropout(dropout[layer]))           \n",
    "        \n",
    "\n",
    "    model.add(Flatten())                          \n",
    "    model.add(Dense(last_features, activation=tf.nn.relu))\n",
    "    model.add(Dropout(last_dropout))\n",
    "    model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fonction <i>save_model</i></h3>\n",
    "<ul>\n",
    "<li>Cette fonction <b style=\"backgroud-color:powerblue\"><i>save_model</i></b> prend un modèle sequentièle et le nom du fichier.</li>\n",
    "<li>Elle appelle <i style=\"color:blue\">keras.models.Sequential.to_json</i> et <i style=\"color:blue\">save_weights</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_json=\"\", file_weight=\"\", workdir=\"\"):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    projet_dir = workdir if len(workdir)>0 else \"C:\\\\Users\\\\jeang\\\\Documents\\\\BDEB\\\\A62\\\\ProSyn3\"\n",
    "    model_filename = 'fer.json' if file_json==\"\" else file_json\n",
    "    with open(os.path.join(projet_dir, model_filename), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    # model.save_weights(\"C:\\\\Users\\\\Nisha Haulkhory\\\\projet de synthese\\\\GIT\\\\ProSyn3\\\\fer.h5\")\n",
    "    weight_filename = 'fer.h5' if file_weight==\"\" else file_weight\n",
    "    model.save_weights(os.path.join(projet_dir, weight_filename))\n",
    "    print(\"Saved model to disk done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fonction <i>show_confusion_matrix</i></h3>\n",
    "<ul>\n",
    "    <li>Cette fonction <b style=\"backgroud-color:powerblue\"><i>show_confusion_matrix</i></b> prend comme entrée un modèle</li>\n",
    "    <li>Elle appelle <i style=\"color:blue\">model.predict</i> et le set de test pour <i>y_pred</i></li>\n",
    "    <li>Elle compare <i>y_pred</i> avec les valeurs réelles pour construire la matrice de confusion.</li>\n",
    "    <li>Elle normalize la matrice de confusion car les cibles dans nos données originales ne sont pas uniformes.</li>\n",
    "    <li>Mais ceci pourrait déclencher des erreurs pour les classes manquantes car Div_by_zero donne NA</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(model, x_test=X_test, y=df_test.emotion):\n",
    "    submission = pd.DataFrame(model.predict(x_test))\n",
    "    submission['label'] = submission.idxmax(axis=1)\n",
    "    cm = confusion_matrix(submission['label'], y)               # df_test['emotion'])\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm = cm.round(2)\n",
    "    return pd.DataFrame(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Class <i>BalancedDataGenerator</i></h3>\n",
    "<p>\n",
    "Cette classe <b style=\"backgroud-color:powerblue\"><i>BalancedDataGenerator</i></b> est trouvée sur web pour imbalanced dataset.<br>Pour en savoir plus sur <b>Keras ImageDataGenerator and Data Augmentation</b><a href=\"https://gist.github.com/arnaldog12/16efc663c869b35e2479bd607d56c1da\"> Cliquer ici.</a>\n",
    "<br>Elle selecte non uniformement les échantillons pour les augmenter et finir avec relativement un dataset uniform.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedDataGenerator(Sequence):\n",
    "    \"\"\"ImageDataGenerator + RandomOversampling\"\"\"\n",
    "    def __init__(self, x, y, datagen, batch_size=32):\n",
    "        self.datagen = datagen\n",
    "        self.batch_size = min(batch_size, x.shape[0])\n",
    "        datagen.fit(x)  \n",
    "        self.gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1), y, \n",
    "                                                                  sampler=RandomOverSampler(), \n",
    "                                                                  batch_size=self.batch_size, keep_sparse=True)\n",
    "        self._shape = (self.steps_per_epoch * batch_size, *x.shape[1:])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_batch, y_batch = self.gen.__next__()\n",
    "        x_batch = x_batch.reshape(-1, *self._shape[1:])\n",
    "        return self.datagen.flow(x_batch, y_batch, batch_size=self.batch_size).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objet <i>ImageDataGenerator</i></h3>\n",
    "<p>Cette instance d'objet <b style=\"backgroud-color:powerblue\"><i>datagen</i></b> est utilisable dans plusieurs cas pour l 'augmentation des données.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                             featurewise_std_normalization=False,\n",
    "                             rotation_range=20,\n",
    "                             width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             shear_range=0.15,\n",
    "                             zoom_range=0.15,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=False)\n",
    "\n",
    "# bgen = BalancedDataGenerator(X_train, Y_train, datagen, batch_size=256)\n",
    "# steps_per_epoch = bgen.steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fonction <i>uniform_data_aug_fit</i></h3>\n",
    "<p><ul>\n",
    "<li>Cette fonction <b style=\"backgroud-color:powerblue\"><i>uniform_data_aug_fit</i></b> prend comme entrées:\n",
    "<ol><li>un modèle à apprendre</li>\n",
    "<li>une instance ImageDataGenerator</li>\n",
    "<li>le set d'apprentissage,</li>\n",
    "<li>le set de validation.</li>\n",
    "<li>et la taille du batch bs</li></ol></li>\n",
    "<li>Elle calcule le nombre de <i style=\"color:blue\">steps_per_epoch</i> pour contrôler le trade-off temps et performance</li>\n",
    "<li>La différence est qu'elle n'utilise pas le même taux d' augmentation, mais plus grand pour les classes moins populaires</li>\n",
    "<li>Elle appelle la fonction <i style=\"color:blue\">model.fit_generator</i> pour entraîner le modèle.</li>\n",
    "</ul></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_data_aug_fit(model, dgen, x_train, y_train, valid, bs, taux=9, ep=10):\n",
    "    taux_augmented_data = min(taux, 48)  \n",
    "    batch_size = min(bs, x_train.shape[0])\n",
    "    steps_per_epoch = int(x_train.shape[0] * taux_augmented_data / batch_size)\n",
    "    l = model.fit_generator(dgen.flow(x_train, y_train, batch_size=batch_size), epochs=ep, validation_data=valid,\n",
    "                            verbose = 1, steps_per_epoch = steps_per_epoch)\n",
    "    return model, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fonction <i>balance_fit</i></h3>\n",
    "<ul>\n",
    "<li>Cette fonction <b style=\"backgroud-color:powerblue\"><i>balance_fit</i></b> prend un modèle et une instance ImageDataGenerator</li>\n",
    "<li>Elle appelle la fonction <i style=\"color:blue\">uniform_data_aug_fit</i> pour faire apprendre le modèle</li>\n",
    "<li>La difference est qu'elle n'utilise pas le même taux d'augmentation, plus grand pour les classes moins populaires</li>\n",
    "<li>En effet, elle entraine 3 fois le modèle avec 3 training set differents </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_fit(model, dgen, valid=(X_valid, Y_valid), bs=32, ep=10, fnm='fer.json', fnw='fer.h5'):\n",
    "    \n",
    "    #train the model with uniform_data_aug_fit but the trainset without the most popular calss 3\n",
    "    train_o = df_training[(df_training.emotion != 3)]\n",
    "    valid_o = df_validation[(df_validation.emotion != 3)]\n",
    "    test_o = df_test[(df_test.emotion != 3)]\n",
    "    \n",
    "    x, y = image_to_4d_array(train_o)\n",
    "    validation = image_to_4d_array(valid_o)\n",
    "    x_test, y_test = image_to_4d_array(test_o)\n",
    "    \n",
    "    model, l = uniform_data_aug_fit(model, dgen, x, y, validation, bs, taux=12, ep=ep)\n",
    "    show_confusion_matrix(model, x_test=x_test, y=test_o.emotion)\n",
    "    sns.lineplot(data=pd.DataFrame(l.history)[['accuracy','val_accuracy']], palette=\"tab10\", linewidth=2.5);\n",
    "    print(\"Evaluating the model on test data ...\")\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    \n",
    "    # second train the model with all train set, validation set and test set without data augmentation\n",
    "    l = model.fit(x=X_train, y=Y_train, validation_data=valid, batch_size=256, epochs=ep)\n",
    "        \n",
    "    return l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fonction <i>show_save_trained_model</i></h3>\n",
    "<ul>\n",
    "<li>Apres avoir entraîné un modèle, nous voulons souvent savoir l'historique d'apprentissage, la performance, et sauvegarder le modèle</li>\n",
    "<li>Cette fonction <b style=\"backgroud-color:powerblue\"><i>show_save_trained_model</i></b> donne une flexibilité pour differentes méthodes d'apprentissage</li>\n",
    "<li>Elle prend le modèle et l'historique d'apprentissage en entrées</li>\n",
    "<li>Elle compare avec les set de test pour évaluer le modèle.</li>\n",
    "<li>Elle imprine aussi la matrice de confusion </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_save_trained_model(model, history, fnm, fnw, x_test=X_test, y_test=Y_test, y=df_test.emotion):\n",
    "    sns.lineplot(data=pd.DataFrame(history.history)[['accuracy','val_accuracy']], palette=\"tab10\", linewidth=2.5);\n",
    "    save_model(model, file_json=fnm, file_weight=fnw)\n",
    "    print(\"Evaluating the model on test data ...\")\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    print(show_confusion_matrix(model, x_test, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fonction <i>fit_test_save_model</i></h3>\n",
    "<ul>\n",
    "<li>Cette fonction <b style=\"backgroud-color:powerblue\"><i>fit_test_save_model</i></b> regroupe plusieurs commandes</li>\n",
    "<li>Elle fournie une interface pour l'apprentissage, l'évaluation, la visualisation, et la sauvegarde du modèle cnn.</li>\n",
    "<li>Si le drapeau <b style=\"backgroud-color:powerblue\"><i>aug</i></b> est Faux, elle apprend le modèle avec le set d apprentissage et le set de validation.</li>\n",
    "<li>Si le drapeau <b style=\"backgroud-color:powerblue\"><i>aug</i></b> est True, elle apprend le modèle avec le set d'apprentissage augmenté.</li>\n",
    "<li>Une fois l'apprentissage se termine, elle execute des commandes pour afficher l'historique d'apprentissage, sauvegarder le modèle.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_test_save_model(model, x_train=X_train, y_train=Y_train, valid=(X_valid, Y_valid), \n",
    "                        aug=True, imbalance=False, dgen = datagen,\n",
    "                        bs=32, ep=10, fnm='fer.json', fnw='fer.h5'):\n",
    "    \n",
    "    print(\"Fitting the model ...\")\n",
    "    batch_size = min(bs, x_train.shape[0])\n",
    "    \n",
    "    if not aug:\n",
    "        l = model.fit(x=x_train, y=y_train, validation_data=valid, batch_size=batch_size, epochs=ep)\n",
    "        \n",
    "    elif imbalance:\n",
    "        # I prefer balance_fit because I know how it works\n",
    "        l = balance_fit(model, dgen, valid, bs=batch_size, ep=ep)\n",
    "        \n",
    "        # I tried this method but I did not get reasonable result: There are something wrong\n",
    "        # bgen = BalancedDataGenerator(x_train, y_train, dgen, batch_size=batch_size)\n",
    "        # steps_per_epoch = bgen.steps_per_epoch\n",
    "        # l = model.fit_generator(bgen, steps_per_epoch, epochs=ep, validation_data=valid, verbose = 1)\n",
    "    \n",
    "    else:\n",
    "        _, l = uniform_data_aug_fit(model, dgen, x_train, y_train, taux=9, bs=batch_size, ep=ep, valid=valid)\n",
    "\n",
    "    show_save_trained_model(model, l, fnm, fnw)\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1er exemple de construction de CNN model</h3>\n",
    "<p>Par defaut, il y a 5 couches conv2D, la taille k_size est (3,3) pour chaque couche</p>\n",
    "<p>Notons que les valeurs dropout sont 0.1 pour ces 5 couches, mais last_dropout(ld) est 0.45</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 986,119\n",
      "Trainable params: 985,159\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Same architecture will used for 3nd eample\n",
    "model = get_model_v23(num_filtre=(16, 32, 64, 128, 256), lf=256, ld=0.45, p_size=(2,2,2,2,0), dropout=(0.1, 0.1, 0.1, 0.1, 0.1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model ...\n",
      "WARNING:tensorflow:From C:\\Users\\hyuma\\.conda\\envs\\Python_ENV\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/25\n",
      "1172/8074 [===>..........................] - ETA: 8:45 - loss: 1.8392 - acc: 0.2449"
     ]
    }
   ],
   "source": [
    "l = fit_test_save_model(model, bs=32, ep=25, fnm='fer238.json', fnw='fer238.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Commentaires</h4>\n",
    "<p>Tant qu'il n'y a pas de sur-apprentissage, un modèle peut être continuellement entraîné jusqu'à la performance desiré. Ceci est vrai au moins sur le même set d'apprentissage.<br> Nous constatons que <i>l'accuracy</i> s'améliore dans chaque epoch, et que <i>val_accuracy</i> a aussi une tendance d'augmenter. Dans chaque epoch, il y a 8074 steps, ceci est definie par la taille de notre set d'apprentissage, le taux d'augmentation, et la batch_size (28709*9/32).<br>Notons que ce nombre d' étapes doit être assez élevé pour un meilleur apprentssage. Mais le temps d'apprentissage augmente aussi rapidement avec ce nombre. Par exemple l'execution de la commande ci-dessus a pris environ 14000 secondes, ou 3.89 heurs.(Ceci est variable selon le type de machine sur laquelle est executé ce code.)</p>\n",
    "<p>Effet nombre filtres: forte nombre filtre est préferable pour chercher features, mais non favorable pour overfitting. Non plus pour le temps d'apprentissage. Le plus important, mettre le nombre de filtre plus elevé dans les couches profondes. Ceci est favorable pour réduire le temps d'apprentissage.</p>\n",
    "<p>Effet dropout: forte dropout est préferable pour contrôler overfitting, mais non favorable pour ralentir la vitesse d'apprentissage. C'est un trade-off dans l'apprentissage par CNN</p>\n",
    "<p>Meilleur performance obtenu avec le set de test avant était de 64.5% lorsque le dropout=(0.1,0.1,0.1,0.1,0.1), ld=0.35, ep=10. Avec les condition ci-dessus, nous avons eu une meilleure performance: <b>66%</b></p>\n",
    "<hr>\n",
    "<h3>Proposition nouveau essai</h3>\n",
    "<p>Pouvons-nous entraîner un modèle avec different set d'appretissage? Si l'apprentissage du modèle s'améliore continuellement malgré le changement du set d'apprentissage, nous pouvons traiter le problème imbalance de notre set d'apprentissage. En effet, la classe la plus populaire <b>happy(3)</b> a une performance d'environ 91% mais la classe <b>sad(4)</b> a seulement 44% selon un matrice de confusion.<br>Si, au lieu d'augmenter tous train set 9 fois, on augmente seulement 12 fois les autre classes moins populaires, on pourrait sauve du temps d'apprentissage.<br>\n",
    "En basant sur cette proposition, nous allons traiter notre défi d'over-fitting et unbalanced dataset ci-dessous.<br>En 3ième exemple, nous allons reprendre cette technique.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_v23(num_filtre=(16, 32, 64, 128, 256), lf=256, ld=0.5, p_size=(2,2,2,2,0), dropout=(0.1, 0.1, 0.1, 0.1, 0.1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o = df_training[(df_training.emotion != 3)]\n",
    "valid_o = df_validation[(df_validation.emotion != 3)]\n",
    "test_o = df_test[(df_test.emotion != 3)]\n",
    "    \n",
    "x, y = image_to_4d_array(train_o)\n",
    "validation = image_to_4d_array(valid_o)\n",
    "x_test, y_test = image_to_4d_array(test_o)\n",
    "    \n",
    "model, l = uniform_data_aug_fit(model, datagen, x, y, valid=validation, bs=32, taux=9, ep=10)\n",
    "show_confusion_matrix(model, x_test=x_test, y=test_o.emotion)\n",
    "sns.lineplot(data=pd.DataFrame(l.history)[['accuracy','val_accuracy']], palette=\"tab10\", linewidth=2.5);\n",
    "print(\"Evaluating the model on test data ...\")\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour corriger les erreur ci-haute\n",
    "show_confusion_matrix(model, x_test=x_test, y=test_o.emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour corriger les erreur ci-haute\n",
    "sns.lineplot(data=pd.DataFrame(l.history)[['accuracy','val_accuracy']], palette=\"tab10\", linewidth=2.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour corriger les erreur ci-haute\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if these data are overwriten\n",
    "#x, y = image_to_4d_array(df_training)\n",
    "#validation = image_to_4d_array(df_validation)\n",
    "#x_test, y_test = image_to_4d_array(df_test)\n",
    "\n",
    "#X_train, Y_train = image_to_4d_array(df_training)\n",
    "#X_test, Y_test = image_to_4d_array(df_test)\n",
    "#X_valid, Y_valid = image_to_4d_array(df_validation)\n",
    "\n",
    "l = model.fit(x=X_train, y=Y_train, validation_data=(X_valid, Y_valid), batch_size=256, epochs=30)    \n",
    "#model, l = uniform_data_aug_fit(model, datagen, x, y, valid=validation, bs=32, taux=9, ep=10)\n",
    "show_confusion_matrix(model, x_test=X_test, y=df_test.emotion)\n",
    "sns.lineplot(data=pd.DataFrame(l.history)[['accuracy','val_accuracy']], palette=\"tab10\", linewidth=2.5);\n",
    "print(\"Evaluating the model on test data ...\")\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(model, x_test=X_test, y=df_test.emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Commentaires sur 2ième apprentissage avec différent set d'apprentissage.</h4>\n",
    "<p>Eventuellement, le changement de data-set pour 2ième apprentissage d'un modele semble être une technique positive. Pour apprendre d'un modèle vierge par <i>uniform_data_augmentation</i>, on a besoin d'environ 14000 (570x25) seconds pour une performance de 64%.</p>\n",
    "<p>Avec un apprentissage par <i>uniform_data_augmentation</i> sur les autres classes que la plus populaire puis un 2ième apprentissage sans data_augmentation, on a besoin d'environ 5900 (430x10+54X30) seconds pour une performance similaire. Un économie de plus que 50%.</p>\n",
    "<hr>\n",
    "<h3>2ième exemple de construction de CNN model</h3>\n",
    "<p>Par défaut, il y a 5 couches, la taille k_size est (3,3) pour chaque couche.<br>\n",
    "    <b>Notons que les valeurs dropout sont 0.25 pour ces 5 couches, et last_dropout(ld) est 0.5</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_v23(num_filtre=(16, 32, 64, 128, 256), lf=256, ld=0.5, p_size=(2,2,2,2,0), dropout=(0.25, 0.25, 0.25, 0.25, 0.25))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2ième exemple d'apprendre d'un CNN model</h3>\n",
    "<p>Cette fois, la technique d'augmentation de données (Data Augmentation) n'est pas utilisée.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = fit_test_save_model(model, bs=256, ep=50, aug=False, fnm='fer234.json', fnw='fer234.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Commentaires</h4>\n",
    "<p>Nous observons un sur-apprentissage malgré une forte 'dropout'. Une performance de 62.7% est obtenue sans l'augmentation des données. Le temps d'apprentissage est relativement courte: environ 2700s (54x50)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3ième exemple d'apprendre un CNN model</h3>\n",
    "<p>Cette fois-ci, nous avons utilisé une technique de deux entraînements sequentiels.Cependant <i>\"imbalanced data augmentation\"</i> est utilisé pendant le premier entraînement uniquement.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_v23(num_filtre=(16, 32, 64, 128, 256), lf=256, ld=0.45, p_size=(2,2,2,2,0), dropout=(0.1, 0.1, 0.1, 0.1, 0.1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = fit_test_save_model(model, bs=32, ep=10, imbalance=True, fnm='fer235.json', fnw='fer235.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Commentaires</h4>\n",
    "<p>Nous observons d'over-fitting seulement pour la deuxième entraînement malgré faible dropout. Une performance de 63.9% est obtenue qui est de même que uniform data augmentation. Le temps d'apprentissage est relativement courte, environ 6000s (555+51)x10. </p>\n",
    "<hr>\n",
    "<h2>Conclusion</h2>\n",
    "<ul>\n",
    "<li>L'interface de construction du model cnn et de l'apprentissage devient simplement deux appels de deux fonctions.</li>\n",
    "<li>Par choix des hyperparametres, on peut avoir differents modèles et la technique d'apprentissage.</li>\n",
    "<li>Sans data augmentation, maximum accuracy sur set de test est 62% en ce moment. Ce n'est pas le plus haute.</li>\n",
    "<li>De plus, sans data augmentation, forte dropout sont nécessaires pour lutter contre le sur-apprentissage.</li>\n",
    "<li>Avec data augmentation, nous obtenous de meilleurs performance: 66% dans le cas keras.ImageDataGenerator.</li>\n",
    "<li>Avec cette technique, faible valeurs dropout sont suffisant pour empêcher un overfitting.</li>\n",
    "<li>mais le temps d'apprentissage est relativement longue.</li>\n",
    "<li>Pour réduire le temps d'apprentissage, nous avons utilisé une technique telle que seul les classes qui sont difficiles à classifier ont eu une data augmentation, le modele est ensuite entrainé par la même technique sans data augmentation. Nous avons eu une performance aussi elevée que 64% mais le temps d'apprentissage est réduit à plus que 50% (6060/14600)</li>\n",
    "<li>Avec une technique oversampling, Nous n'avons pas eu de bon résultat, faut du temps serré et manque de compréhension de cette technique. Voir les cellules qui suivent la conclusion</li>\n",
    "</ul>\n",
    "<hr>\n",
    "<h2>Annexe</h2>\n",
    "<p>Noté sur une mauvaise expérience avec une classe <b><i style=\"color:blue\">BalancedDataGenerator</i></b></p>\n",
    "<hr>\n",
    "<h3>3ième exemple de construction du CNN model</h3>\n",
    "<p>Par défaut, il y a 5 couches, la taille k_size est (3,3) pour chaque couche.<br>\n",
    "Notons que les valeurs dropout sont 0.1 pour les 5 couches, mais last_dropout(ld) est 0.45. Cette architecture a été utilisée pour la premier exemple</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_v23(num_filtre=(16, 32, 64, 128, 256), lf=256, ld=0.45, p_size=(2,2,2,2,0), dropout=(0.1, 0.1, 0.1, 0.1, 0.1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3ième Exemple d'apprendre un CNN model</h3>\n",
    "<p>Cette fois, la technique de imbalanced data augmentation est utilisée</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = fit_test_save_model(model, bs=32, ep=10, imbalance=True, fnm='fer235.json', fnw='fer235.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                             featurewise_std_normalization=False,\n",
    "                             rotation_range=45,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=.2,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True)\n",
    "\n",
    "#bgen = BalancedDataGenerator(X_train, Y_train, datagen, batch_size=256)\n",
    "#steps_per_epoch = bgen.steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Réferences </h2>\n",
    "<p>pour en savoir plus sur <a href=\"https://keras.io/api/preprocessing/image/\">keras image data generator class</a><br>\n",
    "Que ce que c'est <a href=\"https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb\">Imbalanced Data</a><br>\n",
    "Pour savoir comment traiter <b>\"Imbalanced image data\"</b> <a href=\"https://medium.com/analytics-vidhya/how-to-apply-data-augmentation-to-deal-with-unbalanced-datasets-in-20-lines-of-code-ada8521320c9\">Cliquer ici.</a><br>\n",
    "<a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\">Code original</a> pour <b>\"Imbalanced image data\"</b></p>\n",
    "<p>En utilisant <b>ImageDataGenerator</b> avec Keras, nous obtenons des données additionnelles d'apprentissage.De ce fait, vous pourriez initialiser le nombre d'exemplaire  pour l'apprentissage par vous même, par exemple si vous voulez doubler les données d'apprentissage il suffit d'appliquer la formule <b>steps_per_epoch as (original_sample_size*2)/batch_size.</b>.  Pour plus d'infos <a href=\"https://stackoverflow.com/questions/47928347/value-of-steps-per-epoch-passed-to-keras-fit-generator-function\"> cliquez ici </a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
